{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import (Artifact, ClassificationMetrics, Input, Metrics, Output, Dataset, Model,\n",
    "                     component)\n",
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '' # Fill with Service Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = '' # project id\n",
    "REGION='asia-southeast1'\n",
    "BUCKET_URI = f\"gs://{PROJECT_ID}\" # fill with gcs path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"\" #pipeline name\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/taxi_trips\".format(BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/renoldnatasasmita/miniconda3/envs/ml/lib/python3.9/site-packages/kfp/dsl/component_decorator.py:119: FutureWarning: Python 3.7 has reached end-of-life. The default base_image used by the @dsl.component decorator will switch from 'python:3.7' to 'python:3.8' on April 23, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.8.\n",
      "  return component_factory.create_component_from_func(\n"
     ]
    }
   ],
   "source": [
    "@component(\n",
    "        packages_to_install=[\"google-cloud-bigquery\", \"google-cloud-storage\", \"pandas\", \"db-dtypes\"]\n",
    ")\n",
    "def get_cleaned_data(bq_source: str, Clean_Data: Output[Dataset]) -> NamedTuple(\"Outputs\", [(\"path\", str)]):\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    client = bigquery.Client(project='FILL with Project ID', location='asia-southeast1')\n",
    "    QUERY = f\"\"\"\n",
    "        SELECT trip_seconds, trip_miles, pickup_community_area, dropoff_community_area, fare, tolls, extras, tips FROM `{bq_source}`\n",
    "    \"\"\"\n",
    "\n",
    "    df = client.query(QUERY).to_dataframe()\n",
    "    df.to_csv(f'{Clean_Data.path}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "        packages_to_install=[\"scikit-learn\", \"pandas\", \"google-cloud-storage\", \"fsspec\", \"gcsfs\"]\n",
    ")\n",
    "def split_the_data(Clean_Data: Input[Dataset], Train_Data: Output[Dataset], Val_Data: Output[Dataset]):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(f'{Clean_Data.path}.csv')\n",
    "    train, test = train_test_split(df, test_size=0.2)\n",
    "    train.to_csv(f'{Train_Data.path}.csv', index=False)\n",
    "    test.to_csv(f'{Val_Data.path}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "        base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-9:latest\",\n",
    "        packages_to_install=[\"google-cloud-storage\", \"fsspec\", \"gcsfs\"]\n",
    ")\n",
    "def create_tensorflow_model(input_size: int,\n",
    "                            hidden_layer_sizes: int,\n",
    "                            output_size: int ,\n",
    "                            activation_name: str,\n",
    "                            output_activation_name: str,\n",
    "                            Base_Model: Output[Model]):\n",
    "    import tensorflow as tf\n",
    "    tf.random.set_seed(seed=0)\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(input_size,)))\n",
    "    for layer_size in [hidden_layer_sizes]:\n",
    "        model.add(tf.keras.layers.Dense(units=layer_size, activation=activation_name))\n",
    "    # The last layer is left without activation\n",
    "    model.add(tf.keras.layers.Dense(units=output_size, activation=output_activation_name))\n",
    "    tf.keras.models.save_model(model, f'{Base_Model.path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "        base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-9:latest\",\n",
    "        packages_to_install=[\"google-cloud-storage\", \"fsspec\", \"gcsfs\"]\n",
    ")\n",
    "def train_model(\n",
    "        Base_Model: Input[Model],\n",
    "        Train_Data: Input[Dataset],\n",
    "        learning_rate:float,\n",
    "        optimizer_name:str,\n",
    "        loss_function_name:str,\n",
    "        batch_size:int,\n",
    "        label_column_name:str,\n",
    "        random_seed:int,\n",
    "        metric_names:str,\n",
    "        number_of_epochs:int,\n",
    "        Trained_Model: Output[Model]):\n",
    "    import tensorflow as tf\n",
    "    tf.random.set_seed(seed=0)\n",
    "    keras_model = tf.keras.models.load_model(filepath=f'{Base_Model.path}')\n",
    "    optimizer_parameters = {}\n",
    "    optimizer_parameters[\"learning_rate\"] = learning_rate\n",
    "    optimizer_config = {\n",
    "        \"class_name\": optimizer_name,\n",
    "        \"config\": optimizer_parameters,\n",
    "    }\n",
    "    optimizer = tf.keras.optimizers.get(optimizer_config)\n",
    "    loss = tf.keras.losses.get(loss_function_name)\n",
    "    training_dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern=f'{Train_Data.path}.csv',\n",
    "        batch_size=batch_size,\n",
    "        label_name=label_column_name,\n",
    "        header=True,\n",
    "        # Need to specify num_epochs=1 otherwise the training becomes infinite\n",
    "        num_epochs=1,\n",
    "        shuffle=True,\n",
    "        shuffle_seed=random_seed,\n",
    "        ignore_errors=True,\n",
    "    )\n",
    "    def stack_feature_batches(features_batch, labels_batch):\n",
    "        # Need to stack individual feature columns to create a single feature tensor\n",
    "        # Need to cast all column tensor types to float to prevent error:\n",
    "        # TypeError: Tensors in list passed to 'values' of 'Pack' Op have types [int32, float32, float32, int32, int32] that don't all match.\n",
    "        list_of_feature_batches = list(tf.cast(x=feature_batch, dtype=tf.float32) for feature_batch in features_batch.values())\n",
    "        return tf.stack(list_of_feature_batches, axis=-1), labels_batch\n",
    "\n",
    "    training_dataset = training_dataset.map(stack_feature_batches)\n",
    "    if metric_names == '':\n",
    "        metric_names = None\n",
    "    keras_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metric_names,\n",
    "    )\n",
    "    keras_model.fit(\n",
    "        training_dataset,\n",
    "        epochs=number_of_epochs,\n",
    "    )\n",
    "\n",
    "    tf.keras.models.save_model(keras_model, f'{Trained_Model.path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "        base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-9:latest\",\n",
    "        packages_to_install=[\"numpy\", \"google-cloud-storage\", \"fsspec\", \"gcsfs\"]\n",
    ")\n",
    "def test_model(\n",
    "        Trained_Model: Input[Model],\n",
    "        Val_Data: Input[Dataset],\n",
    "        label_column_name:str,\n",
    "        batch_size:int,\n",
    "        metrics: Output[Metrics]):\n",
    "    import tensorflow as tf\n",
    "    model = tf.saved_model.load(export_dir=f'{Trained_Model.path}')\n",
    "\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern=f'{Val_Data.path}.csv',\n",
    "        batch_size=batch_size,\n",
    "        label_name=label_column_name,\n",
    "        header=True,\n",
    "        num_epochs=1,\n",
    "        shuffle=False,\n",
    "        ignore_errors=False,\n",
    "    )\n",
    "\n",
    "    def stack_feature_batches(features_batch, labels_batch):\n",
    "        # Need to stack individual feature columns to create a single feature tensor\n",
    "        # Need to cast all column tensor types to float to prevent error:\n",
    "        # TypeError: Tensors in list passed to 'values' of 'Pack' Op have types [int32, float32, float32, int32, int32] that don't all match.\n",
    "        list_of_feature_batches = list(tf.cast(x=feature_batch, dtype=tf.float32) for feature_batch in features_batch.values())\n",
    "        return tf.stack(list_of_feature_batches, axis=-1), labels_batch\n",
    "    dataset_with_label = dataset.map(stack_feature_batches)\n",
    "    mse = tf.keras.metrics.MeanSquaredError()\n",
    "    rmse = tf.keras.metrics.RootMeanSquaredError()\n",
    "    mae  = tf.keras.metrics.MeanAbsoluteError()\n",
    "    for features_batch, labels_batch in dataset_with_label:  # Assuming you have labels in your dataset\n",
    "        predictions_tensor = model(features_batch)\n",
    "        \n",
    "        # Update metrics\n",
    "        mse.update_state(labels_batch, predictions_tensor)\n",
    "        rmse.update_state(labels_batch, predictions_tensor)\n",
    "        mae.update_state(labels_batch, predictions_tensor)\n",
    "\n",
    "    # Get the results\n",
    "    mse_result = mse.result().numpy()\n",
    "    rmse_result = rmse.result().numpy()\n",
    "    mae_result = mae.result().numpy()\n",
    "    metrics.log_metric(\"mean_absolute_error\", float(mae_result))\n",
    "    metrics.log_metric(\"mean_squared_error\", float(mse_result))\n",
    "    metrics.log_metric(\"root_mean_squared_error\", float(rmse_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "        base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-9:latest\",\n",
    "        packages_to_install=[\"google-cloud-aiplatform\", \"google-cloud-storage\", \"fsspec\", \"gcsfs\"]\n",
    ")\n",
    "def upload_model_to_vertex_ai(\n",
    "    Trained_Model: Input[Model],\n",
    "    use_gpu: bool,\n",
    "    display_name: str,\n",
    "    description: str,\n",
    "    project: str,\n",
    "    location: str,\n",
    ")-> NamedTuple(\"Outputs\", [(\"resource_name\", str)]):\n",
    "    import json\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    model = aiplatform.Model.upload_tensorflow_saved_model(\n",
    "        saved_model_dir=f'{Trained_Model.path}',\n",
    "        tensorflow_version=\"2.9\",\n",
    "        use_gpu=use_gpu,\n",
    "\n",
    "        display_name=display_name,\n",
    "        description=description,\n",
    "\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    model_json = json.dumps(model.to_dict(), indent=2)\n",
    "    print(model_json)\n",
    "    resource_name = model.resource_name\n",
    "    return (resource_name,)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "        base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-9:latest\",\n",
    "        packages_to_install=[\"google-cloud-aiplatform\", \"google-cloud-storage\", \"fsspec\", \"gcsfs\"]\n",
    ")\n",
    "def deploy_model_to_vertex_ai_endpoint(\n",
    "          model_name: str,\n",
    "          endpoint_name: str = None,\n",
    "          machine_type: str = \"n1-standard-2\",\n",
    "          min_replica_count: int = 1,\n",
    "          max_replica_count: int = 1,\n",
    "          accelerator_type: str = None,\n",
    "          accelerator_count: str = None,\n",
    "      )-> NamedTuple(\"Outputs\", [(\"endpoint_name\", str)]):\n",
    "        import json\n",
    "        from google.cloud import aiplatform\n",
    "\n",
    "        model = aiplatform.Model(model_name=model_name)\n",
    "\n",
    "        if endpoint_name:\n",
    "            endpoint = aiplatform.Endpoint(endpoint_name=endpoint_name)\n",
    "        else:\n",
    "            endpoint_display_name = model.display_name[:118] + \"_endpoint\"\n",
    "            endpoint = aiplatform.Endpoint.create(\n",
    "                display_name=endpoint_display_name,\n",
    "                project=model.project,\n",
    "                location=model.location,\n",
    "            )\n",
    "\n",
    "        endpoint = model.deploy(\n",
    "            endpoint=endpoint,\n",
    "            # deployed_model_display_name=deployed_model_display_name,\n",
    "            machine_type=machine_type,\n",
    "            min_replica_count=min_replica_count,\n",
    "            max_replica_count=max_replica_count,\n",
    "            accelerator_type=accelerator_type,\n",
    "            accelerator_count=accelerator_count,\n",
    "            # service_account=service_account,\n",
    "            # explanation_metadata=explanation_metadata,\n",
    "            # explanation_parameters=explanation_parameters,\n",
    "            # encryption_spec_key_name=encryption_spec_key_name,\n",
    "        )\n",
    "\n",
    "        endpoint_json = json.dumps(endpoint.to_dict(), indent=2)\n",
    "        print(endpoint_json)\n",
    "        endpoint_name = endpoint.resource_name\n",
    "        return (endpoint_name,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=PIPELINE_NAME, pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    bigquery_dataset: str,\n",
    "    MODEL_DISPLAY_NAME: str,\n",
    "    MACHINE_TYPE: str,\n",
    "    project: str,\n",
    "    gcp_region: str,\n",
    "):\n",
    "    # Get the Data\n",
    "    get_cleaned_data_op = get_cleaned_data(bq_source=bigquery_dataset)\n",
    "\n",
    "    # Split the Data into Train and Test\n",
    "    splitted_data_op = split_the_data(Clean_Data=get_cleaned_data_op.outputs['Clean_Data'])\n",
    "\n",
    "    # Build Tensorflow Model\n",
    "    base_model = create_tensorflow_model(input_size=7,\n",
    "                            hidden_layer_sizes=10,\n",
    "                            output_size=1,\n",
    "                            activation_name='elu',\n",
    "                            output_activation_name='sigmoid')\n",
    "    \n",
    "    # Train the Model\n",
    "    trained_model = train_model(Base_Model=base_model.outputs['Base_Model'],\n",
    "                                Train_Data=splitted_data_op.outputs['Train_Data'],\n",
    "                                learning_rate=0.1,\n",
    "                                optimizer_name='Adadelta',\n",
    "                                loss_function_name='mean_squared_error',\n",
    "                                batch_size=32,\n",
    "                                label_column_name='tips',\n",
    "                                random_seed=0,\n",
    "                                metric_names='',\n",
    "                                number_of_epochs=1000)\n",
    "    \n",
    "    test_model(Trained_Model=trained_model.outputs['Trained_Model'],\n",
    "               Val_Data=splitted_data_op.outputs['Val_Data'],\n",
    "               label_column_name='tips',\n",
    "               batch_size= 1)\n",
    "\n",
    "    deploy_to_model_registry = upload_model_to_vertex_ai(Trained_Model=trained_model.outputs['Trained_Model'],\n",
    "                        use_gpu=False, \n",
    "                        display_name=MODEL_DISPLAY_NAME,\n",
    "                        description='',\n",
    "                        project=project,\n",
    "                        location=gcp_region\n",
    "                        )\n",
    "    \n",
    "    deploy_model_to_vertex_ai_endpoint(model_name = deploy_to_model_registry.outputs['resource_name'], \n",
    "                                                            machine_type= MACHINE_TYPE)\n",
    "    \n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"demo-dataset-1-pipeline-ml.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display-names for Vertex AI resources\n",
    "PIPELINE_DISPLAY_NAME = \"demo-dataset-1-loan-data-pipeline\"  # @param {type:\"string\"}\n",
    "DATASET_DISPLAY_NAME = \"demo-dataset-1-loan-data-dataset\"  # @param {type:\"string\"}\n",
    "MODEL_DISPLAY_NAME = \"demo-dataset-1-loan-data-model\"  # @param {type:\"string\"}\n",
    "TRAINING_DISPLAY_NAME = \"demo-dataset-1-loan-data-training\"  # @param {type:\"string\"}\n",
    "ENDPOINT_DISPLAY_NAME = \"demo-dataset-1-loan-data-endpoint\"  # @param {type:\"string\"}\n",
    "\n",
    "# Otherwise, use the default display-names\n",
    "if PIPELINE_DISPLAY_NAME == \"demo-dataset-1-loan-data-pipeline\":\n",
    "    PIPELINE_DISPLAY_NAME = f\"pipeline_demo1_{UUID}\"\n",
    "\n",
    "if DATASET_DISPLAY_NAME == \"demo-dataset-1-loan-data-dataset\":\n",
    "    DATASET_DISPLAY_NAME = f\"dataset_demo1_{UUID}\"\n",
    "\n",
    "if MODEL_DISPLAY_NAME == \"demo-dataset-1-loan-data-model\":\n",
    "    MODEL_DISPLAY_NAME = f\"model_demo1_{UUID}\"\n",
    "\n",
    "if TRAINING_DISPLAY_NAME == \"demo-dataset-1-loan-data-training\":\n",
    "    TRAINING_DISPLAY_NAME = f\"automl_training_demo1_{UUID}\"\n",
    "\n",
    "if ENDPOINT_DISPLAY_NAME == \"demo-dataset-1-loan-data-endpoint\":\n",
    "    ENDPOINT_DISPLAY_NAME = f\"endpoint_demo1_{UUID}\"\n",
    "\n",
    "# Set machine type\n",
    "MACHINE_TYPE = \"n1-standard-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region validated: asia-southeast1\n"
     ]
    }
   ],
   "source": [
    "# Validate region of the given source (BigQuery) against region of the pipeline\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '' # fill with the service account\n",
    "\n",
    "bq_source = \"\" #bq source project_id.dataset.table\n",
    "\n",
    "client = bigquery.Client(location='asia-southeast1')\n",
    "bq_region = client.get_table(bq_source).location.lower()\n",
    "try:\n",
    "    assert bq_region in REGION\n",
    "    print(f\"Region validated: {REGION}\")\n",
    "except AssertionError:\n",
    "    print(\n",
    "        \"Please make sure the region of BigQuery (source) and that of the pipeline are the same.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the pipeline\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=PIPELINE_DISPLAY_NAME,\n",
    "    template_path=\"demo-dataset-1-pipeline-ml.yaml\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        \"project\": PROJECT_ID,\n",
    "        \"gcp_region\": REGION,\n",
    "        \"bigquery_dataset\": f\"{bq_source}\",\n",
    "        \"MODEL_DISPLAY_NAME\": MODEL_DISPLAY_NAME,\n",
    "        \"MACHINE_TYPE\": MACHINE_TYPE,\n",
    "    },\n",
    "    enable_caching=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/8457519537/locations/asia-southeast1/pipelineJobs/tabular-demo-dataset-1-training-singapore-20240620140420\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/8457519537/locations/asia-southeast1/pipelineJobs/tabular-demo-dataset-1-training-singapore-20240620140420')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/asia-southeast1/pipelines/runs/tabular-demo-dataset-1-training-singapore-20240620140420?project=8457519537\n",
      "PipelineJob projects/8457519537/locations/asia-southeast1/pipelineJobs/tabular-demo-dataset-1-training-singapore-20240620140420 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n",
      "PipelineJob projects/8457519537/locations/asia-southeast1/pipelineJobs/tabular-demo-dataset-1-training-singapore-20240620140420 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/8457519537/locations/asia-southeast1/pipelineJobs/tabular-demo-dataset-1-training-singapore-20240620140420 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/8457519537/locations/asia-southeast1/pipelineJobs/tabular-demo-dataset-1-training-singapore-20240620140420 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/8457519537/locations/asia-southeast1/pipelineJobs/tabular-demo-dataset-1-training-singapore-20240620140420 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/8457519537/locations/asia-southeast1/pipelineJobs/tabular-demo-dataset-1-training-singapore-20240620140420 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/8457519537/locations/asia-southeast1/pipelineJobs/tabular-demo-dataset-1-training-singapore-20240620140420 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/8457519537/locations/asia-southeast1/pipelineJobs/tabular-demo-dataset-1-training-singapore-20240620140420 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/8457519537/locations/asia-southeast1/pipelineJobs/tabular-demo-dataset-1-training-singapore-20240620140420 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/8457519537/locations/asia-southeast1/pipelineJobs/tabular-demo-dataset-1-training-singapore-20240620140420 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/8457519537/locations/asia-southeast1/pipelineJobs/tabular-demo-dataset-1-training-singapore-20240620140420 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/8457519537/locations/asia-southeast1/pipelineJobs/tabular-demo-dataset-1-training-singapore-20240620140420 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/8457519537/locations/asia-southeast1/pipelineJobs/tabular-demo-dataset-1-training-singapore-20240620140420 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/8457519537/locations/asia-southeast1/pipelineJobs/tabular-demo-dataset-1-training-singapore-20240620140420 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/8457519537/locations/asia-southeast1/pipelineJobs/tabular-demo-dataset-1-training-singapore-20240620140420 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/8457519537/locations/asia-southeast1/pipelineJobs/tabular-demo-dataset-1-training-singapore-20240620140420 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/8457519537/locations/asia-southeast1/pipelineJobs/tabular-demo-dataset-1-training-singapore-20240620140420 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/8457519537/locations/asia-southeast1/pipelineJobs/tabular-demo-dataset-1-training-singapore-20240620140420\n"
     ]
    }
   ],
   "source": [
    "# Run the job\n",
    "job.run(service_account='FILL with Service Account email')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
